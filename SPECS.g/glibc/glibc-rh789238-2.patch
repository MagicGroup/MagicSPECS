diff -rup c/malloc/arena.c d/malloc/arena.c
--- c/malloc/arena.c	2012-04-02 21:21:08.842818529 -0600
+++ d/malloc/arena.c	2012-04-02 21:22:25.943306724 -0600
@@ -121,14 +121,14 @@ int __malloc_initialized = -1;
   if(ptr) \
     (void)mutex_lock(&ptr->mutex); \
   else \
-    ptr = arena_get2(ptr, (size)); \
+    ptr = arena_get2(ptr, (size), false); \
 } while(0)
 #else
 # define arena_lock(ptr, size) do { \
   if(ptr && !mutex_trylock(&ptr->mutex)) { \
     THREAD_STAT(++(ptr->stat_lock_direct)); \
   } else \
-    ptr = arena_get2(ptr, (size)); \
+    ptr = arena_get2(ptr, (size), false); \
 } while(0)
 #endif
 
@@ -782,7 +782,7 @@ get_free_list (void)
 
 
 static mstate
-reused_arena (void)
+reused_arena (bool retrying)
 {
   mstate result;
   static mstate next_to_use;
@@ -799,6 +799,15 @@ reused_arena (void)
     }
   while (result != next_to_use);
 
+  /* If we are retrying due to a failure to allocate in the main
+     arena, don't wait for the main arena to become available, select
+     another.
+
+     To really fix this right we would have to try the allocation
+     in every other arena, but that seems like severe overkill.  */
+  if (retrying && result == &main_arena)
+    result = result->next;
+
   /* No arena available.  Wait for the next in line.  */
   (void)mutex_lock(&result->mutex);
 
@@ -813,7 +822,7 @@ reused_arena (void)
 
 static mstate
 internal_function
-arena_get2(mstate a_tsd, size_t size)
+arena_get2(mstate a_tsd, size_t size, bool retrying)
 {
   mstate a;
 
@@ -858,7 +867,7 @@ arena_get2(mstate a_tsd, size_t size)
 	    catomic_decrement (&narenas);
 	}
       else
-	a = reused_arena ();
+	a = reused_arena (retrying);
     }
 #else
   if(!a_tsd)
diff -rup c/malloc/malloc.c d/malloc/malloc.c
--- c/malloc/malloc.c	2012-04-02 21:21:08.984817776 -0600
+++ d/malloc/malloc.c	2012-04-02 21:21:39.533655779 -0600
@@ -2938,7 +2938,7 @@ public_mALLOc(size_t bytes)
       /* ... or sbrk() has failed and there is still a chance to mmap() */
       mstate prev = ar_ptr->next ? ar_ptr : 0;
       (void)mutex_unlock(&ar_ptr->mutex);
-      ar_ptr = arena_get2(prev, bytes);
+      ar_ptr = arena_get2(prev, bytes, true);
       if(ar_ptr) {
 	victim = _int_malloc(ar_ptr, bytes);
 	(void)mutex_unlock(&ar_ptr->mutex);
@@ -3117,7 +3117,7 @@ public_mEMALIGn(size_t alignment, size_t
       /* ... or sbrk() has failed and there is still a chance to mmap() */
       mstate prev = ar_ptr->next ? ar_ptr : 0;
       (void)mutex_unlock(&ar_ptr->mutex);
-      ar_ptr = arena_get2(prev, bytes);
+      ar_ptr = arena_get2(prev, bytes, true);
       if(ar_ptr) {
 	p = _int_memalign(ar_ptr, alignment, bytes);
 	(void)mutex_unlock(&ar_ptr->mutex);
@@ -3164,7 +3164,7 @@ public_vALLOc(size_t bytes)
       /* ... or sbrk() has failed and there is still a chance to mmap() */
       mstate prev = ar_ptr->next ? ar_ptr : 0;
       (void)mutex_unlock(&ar_ptr->mutex);
-      ar_ptr = arena_get2(prev, bytes);
+      ar_ptr = arena_get2(prev, bytes, true);
       if(ar_ptr) {
 	p = _int_memalign(ar_ptr, pagesz, bytes);
 	(void)mutex_unlock(&ar_ptr->mutex);
@@ -3211,7 +3211,7 @@ public_pVALLOc(size_t bytes)
       /* ... or sbrk() has failed and there is still a chance to mmap() */
       mstate prev = ar_ptr->next ? ar_ptr : 0;
       (void)mutex_unlock(&ar_ptr->mutex);
-      ar_ptr = arena_get2(prev, bytes + 2*pagesz + MINSIZE);
+      ar_ptr = arena_get2(prev, bytes + 2*pagesz + MINSIZE, true);
       if(ar_ptr) {
 	p = _int_memalign(ar_ptr, pagesz, rounded_bytes);
 	(void)mutex_unlock(&ar_ptr->mutex);
@@ -3298,7 +3298,7 @@ public_cALLOc(size_t n, size_t elem_size
       /* ... or sbrk() has failed and there is still a chance to mmap() */
       mstate prev = av->next ? av : 0;
       (void)mutex_unlock(&av->mutex);
-      av = arena_get2(prev, sz);
+      av = arena_get2(prev, sz, true);
       if(av) {
 	mem = _int_malloc(av, sz);
 	(void)mutex_unlock(&av->mutex);
